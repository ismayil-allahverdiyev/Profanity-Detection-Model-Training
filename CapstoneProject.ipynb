{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d0e1b0d3-ca74-4af9-be88-7bf3f8010ace",
      "metadata": {},
      "source": [
        "# Capstone Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ab67b3-2aee-4684-82ba-d44d3a0621b8",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7ce816-6d03-4bf0-b1af-f9a565970d08",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from spellchecker import SpellChecker\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import json\n",
        "import ast\n",
        "\n",
        "!pip install pyspellchecker==0.5.6\n",
        "\n",
        "from spellchecker import SpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76209026-8618-4c53-8db7-fab23e84c1ea",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "spell = SpellChecker()\n",
        "lmtzr = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17788f87-3985-4889-9e01-2ef65c201ed9",
      "metadata": {},
      "source": [
        "## Functions to handle recurring strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffaa8d64-115f-474e-be34-3cf122d740fd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def remove_stopwords_from_stringified_list(words):\n",
        "    word_list = ast.literal_eval(words)\n",
        "    filtered_words = [word for word in word_list if word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "def remove_stopwords(word_list):\n",
        "    filtered_words = [word for word in word_list.split(\" \") if word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "def isInteger(s):\n",
        "    try: \n",
        "        int(s)\n",
        "    except ValueError:\n",
        "        return False\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "def remove_numerals(words):\n",
        "    word_list = words.split(\" \")\n",
        "    filtered_words = [word for word in word_list if not isInteger(word)]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "def remove_users(word):\n",
        "    filtered_word = \"\"\n",
        "    for check in word.split():\n",
        "        if \"@\" not in check:\n",
        "            filtered_word += check\n",
        "            filtered_word += \" \"\n",
        "    return filtered_word\n",
        "\n",
        "def spell_check(word_list):\n",
        "    filtered_words = [word if spell.correction(word) is None else spell.correction(word) for word in word_list]\n",
        "    print(filtered_words)\n",
        "    return filtered_words\n",
        "\n",
        "def pos_tagger(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "\n",
        "def lmtzr_data(word_list):\n",
        "    long_word = \"\"\n",
        "    for word in word_list:\n",
        "        long_word = long_word + word + \" \"\n",
        "    pos_tagged = nltk.pos_tag(nltk.word_tokenize(long_word))  \n",
        "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:\n",
        "            lemmatized_sentence.append(lmtzr.lemmatize(word, tag))\n",
        "    lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
        "    return lemmatized_sentence\n",
        "\n",
        "def listToString(list):\n",
        "    return \" \".join(list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e641eedd-b949-44ba-847b-06dc9d5d6e38",
      "metadata": {},
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cf97919-1702-4526-afb3-233636022513",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"labeled_data.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9ed600-b20f-4717-a470-fb2b20465105",
      "metadata": {},
      "source": [
        "### Cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e41e6fa-09de-482e-897c-2c104822a09a",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(remove_users)\n",
        "\n",
        "df[\"tokenized_tweet\"] = df[\"tweet\"].map(tokenizer.tokenize)\n",
        "\n",
        "df[\"tokenized_tweet\"] = df[\"tokenized_tweet\"].apply(listToString)\n",
        "\n",
        "df[\"stopword_removed_tweet\"] = df[\"tokenized_tweet\"].apply(remove_stopwords)\n",
        "\n",
        "df[\"spell_fixed_tweet\"] = df[\"stopword_removed_tweet\"].apply(lambda x : spell_check(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b802ffd-0f82-4f9a-a345-ed444516b846",
      "metadata": {},
      "source": [
        "### Saving the clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cd213f8-4f65-4724-8461-ed00ccedf8df",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "new_df = pd.read_excel(\"spell_checked_data.xlsx\")\n",
        "\n",
        "print(new_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0c789b8-f6d3-4e94-ad80-cc2c7e1b8a34",
      "metadata": {},
      "source": [
        "### Using Lemmatization to get the root of the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48be65ae-7c4b-4ccf-a90c-ed6330b16e14",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(new_df.columns)\n",
        "\n",
        "new_df[\"lemmatized_tweet\"] = new_df[\"spell_fixed_tweet\"].apply(lmtzr_data)\n",
        "\n",
        "new_df[\"lemmatized_tweet\"].apply(remove_numerals)\n",
        "\n",
        "new_df.to_excel(\"spell_checked_data.xlsx\", index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1a96c7b-7542-408f-b9b3-62f60c6cecdf",
      "metadata": {},
      "source": [
        "### Splitting the training and the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7997fa37-79a9-474d-b943-a5e61cac91cd",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(new_df['lemmatized_tweet'], new_df['class'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c38851c2-6d62-4647-a6f9-f578305c2b98",
      "metadata": {},
      "source": [
        "### Using TfidfVectorizer to transform the data into vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9336625-6577-403b-bf89-d11c4550a8f4",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d96e21db",
      "metadata": {},
      "source": [
        "### Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c969d8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline_lr = Pipeline([\n",
        "    ('tfidf', vectorizer),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline_svc = Pipeline([\n",
        "    ('tfidf', vectorizer),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "\n",
        "pipeline_rf = Pipeline([\n",
        "    ('tfidf', vectorizer),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9944c81",
      "metadata": {},
      "source": [
        "### Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e857d54",
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_cross_validation(pipeline, X, y):\n",
        "    scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')\n",
        "    print(\"Cross-validation scores:\", scores)\n",
        "    print(\"Mean Accuracy:\", scores.mean())\n",
        "    print(\"Standard Deviation of Accuracy:\", scores.std())\n",
        "\n",
        "# Performing cross-validation for each pipeline\n",
        "print(\"Logistic Regression:\")\n",
        "custom_cross_validation(pipeline_lr, X_train, y_train)\n",
        "\n",
        "print(\"SVM:\")\n",
        "custom_cross_validation(pipeline_svc, X_train, y_train)\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "custom_cross_validation(pipeline_rf, X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22be1a15-25a1-41c2-ac39-df5f80168461",
      "metadata": {},
      "source": [
        "### Using Logistic regression to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b83f51da-a62a-4c55-a1b3-2ea870a83f89",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pipeline_lr.fit(X_train, y_train)\n",
        "y_pred_lr = pipeline_lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d841892b-eb55-430d-9520-eedfb2479f03",
      "metadata": {},
      "source": [
        "### Using Support Vector Classification to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caba9be2-4db8-44c0-ab94-6d7654884f22",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pipeline_svc.fit(X_train, y_train)\n",
        "y_pred_svc = pipeline_svc.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db03e3f4-fc56-45d1-bce9-7ca3baf887bb",
      "metadata": {},
      "source": [
        "### Using Random Forest to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b887230-6410-4061-82f5-72da73b0023e",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pipeline_rf.fit(X_train, y_train)\n",
        "y_pred_rf = pipeline_rf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2837376e-5d4f-4ea4-8a59-a53be0408c22",
      "metadata": {},
      "source": [
        "### Evaluating the prediction results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51a360aa-65fa-41e1-a193-e95cc388a48b",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svc))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8576d025-09f6-42a0-94c8-66ebbd0cdcc7",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
